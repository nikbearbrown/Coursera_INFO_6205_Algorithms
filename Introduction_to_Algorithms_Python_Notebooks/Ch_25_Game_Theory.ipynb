{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Best Response Dynamics"
      ],
      "metadata": {
        "id": "SUGqUERjhNlf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SufvIo-ehDZ0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define payoff matrices\n",
        "payoff_matrix_1 = np.array([[2, 0], [1, 3]])\n",
        "payoff_matrix_2 = np.array([[1, 0], [0, 2]])\n",
        "\n",
        "# Initialize strategies\n",
        "strategies = [0, 0]\n",
        "\n",
        "def best_response(player, opponent_strategy):\n",
        "    if player == 0:\n",
        "        return np.argmax(payoff_matrix_1[:, opponent_strategy])\n",
        "    else:\n",
        "        return np.argmax(payoff_matrix_2[:, opponent_strategy])\n",
        "\n",
        "# Best Response Dynamics\n",
        "for _ in range(10):\n",
        "    strategies[0] = best_response(0, strategies[1])\n",
        "    strategies[1] = best_response(1, strategies[0])\n",
        "\n",
        "print(\"Nash Equilibrium strategies:\", strategies)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prisoner's Dilemma Strategy"
      ],
      "metadata": {
        "id": "btQTNZ3FhX20"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prisoners_dilemma_strategy(player1_choice, player2_choice, T, R, P, S):\n",
        "    if player1_choice == 'D' and player2_choice == 'C':\n",
        "        payoff_player1 = T\n",
        "        payoff_player2 = S\n",
        "    elif player1_choice == 'C' and player2_choice == 'D':\n",
        "        payoff_player1 = S\n",
        "        payoff_player2 = T\n",
        "    elif player1_choice == 'C' and player2_choice == 'C':\n",
        "        payoff_player1 = R\n",
        "        payoff_player2 = R\n",
        "    else:  # Both choose 'D'\n",
        "        payoff_player1 = P\n",
        "        payoff_player2 = P\n",
        "\n",
        "    return payoff_player1, payoff_player2\n",
        "\n",
        "# Example usage:\n",
        "T = 5  # Temptation to defect\n",
        "R = 3  # Reward for mutual cooperation\n",
        "P = 1  # Punishment for mutual defection\n",
        "S = 0  # Sucker's payoff\n",
        "\n",
        "# Players' choices ('C' for Cooperate, 'D' for Defect)\n",
        "player1_choice = 'C'\n",
        "player2_choice = 'D'\n",
        "\n",
        "# Calculate payoffs based on choices\n",
        "payoff_player1, payoff_player2 = prisoners_dilemma_strategy(player1_choice, player2_choice, T, R, P, S)\n",
        "\n",
        "# Print results\n",
        "print(f\"Player 1 payoff: {payoff_player1}\")\n",
        "print(f\"Player 2 payoff: {payoff_player2}\")"
      ],
      "metadata": {
        "id": "S1UQGlnKhYLe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Battle of the Sexes Strategy"
      ],
      "metadata": {
        "id": "eNYW5UomhhV7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def battle_of_the_sexes_strategy(a, b, c):\n",
        "    # Calculate probabilities for mixed strategies\n",
        "    p = (b - c) / (a + b - 2 * c)\n",
        "    q = (a - c) / (a + b - 2 * c)\n",
        "\n",
        "    # Determine Player 1's choice\n",
        "    if random.random() < p:\n",
        "        player1_choice = 'A'\n",
        "    else:\n",
        "        player1_choice = 'B'\n",
        "\n",
        "    # Determine Player 2's choice\n",
        "    if random.random() < q:\n",
        "        player2_choice = 'A'\n",
        "    else:\n",
        "        player2_choice = 'B'\n",
        "\n",
        "    return player1_choice, player2_choice\n",
        "\n",
        "# Example usage:\n",
        "a = 3  # Payoff for Player 1 if both choose A\n",
        "b = 2  # Payoff for Player 1 if both choose B\n",
        "c = 1  # Payoff for Player 2 if both choose different activities\n",
        "\n",
        "# Calculate choices based on strategy\n",
        "player1_choice, player2_choice = battle_of_the_sexes_strategy(a, b, c)\n",
        "\n",
        "# Print results\n",
        "print(f\"Player 1 choice: {player1_choice}\")\n",
        "print(f\"Player 2 choice: {player2_choice}\")"
      ],
      "metadata": {
        "id": "FE09ZvlNhhrd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chicken Game Strategy"
      ],
      "metadata": {
        "id": "bS4wqsj7hrB_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def chicken_game_strategy(C, S):\n",
        "    # Calculate mixed strategy probabilities\n",
        "    p = C / (C + S)\n",
        "\n",
        "    # Determine Player 1's choice\n",
        "    if random.random() < p:\n",
        "        player1_choice = 'Swerve'\n",
        "    else:\n",
        "        player1_choice = 'Continue'\n",
        "\n",
        "    # Determine Player 2's choice\n",
        "    if random.random() < p:\n",
        "        player2_choice = 'Swerve'\n",
        "    else:\n",
        "        player2_choice = 'Continue'\n",
        "\n",
        "    return player1_choice, player2_choice\n",
        "\n",
        "# Example usage:\n",
        "C = 3  # Payoff for Player 1 if they both swerve\n",
        "S = 2  # Payoff for Player 1 if Player 2 continues and they swerve\n",
        "\n",
        "# Calculate choices based on strategy\n",
        "player1_choice, player2_choice = chicken_game_strategy(C, S)\n",
        "\n",
        "# Print results\n",
        "print(f\"Player 1 choice: {player1_choice}\")\n",
        "print(f\"Player 2 choice: {player2_choice}\")"
      ],
      "metadata": {
        "id": "JSzJ5zc6hrY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rubinstein Bargaining Model"
      ],
      "metadata": {
        "id": "MBkEpXQOh2YW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def rubinstein_bargaining_model(delta_A, delta_B, x_A0, x_B0, rounds):\n",
        "    x_A = x_A0\n",
        "    x_B = x_B0\n",
        "\n",
        "    for t in range(rounds):\n",
        "        if t % 2 == 0:  # Player A's turn\n",
        "            x_A = random.uniform(x_A, delta_A)\n",
        "            if random.random() < acceptance_probability(x_A, x_B, delta_A, delta_B):\n",
        "                return x_A  # Agreement reached\n",
        "            else:\n",
        "                x_B = random.uniform(x_B, delta_B)\n",
        "        else:  # Player B's turn\n",
        "            x_B = random.uniform(x_B, delta_B)\n",
        "            if random.random() < acceptance_probability(x_B, x_A, delta_B, delta_A):\n",
        "                return x_B  # Agreement reached\n",
        "            else:\n",
        "                x_A = random.uniform(x_A, delta_A)\n",
        "\n",
        "    return None  # No agreement reached within the specified rounds\n",
        "\n",
        "def acceptance_probability(offer, opposing_offer, delta_self, delta_opponent):\n",
        "    return (delta_opponent - (opposing_offer - offer)) / (delta_self + delta_opponent)\n",
        "\n",
        "# Example usage:\n",
        "delta_A = 10  # Reservation value for Player A\n",
        "delta_B = 8   # Reservation value for Player B\n",
        "x_A0 = 5      # Initial offer from Player A\n",
        "x_B0 = 6      # Initial offer from Player B\n",
        "rounds = 100  # Maximum number of negotiation rounds\n",
        "\n",
        "agreement = rubinstein_bargaining_model(delta_A, delta_B, x_A0, x_B0, rounds)\n",
        "if agreement is not None:\n",
        "    print(f\"Agreement reached at {agreement}\")\n",
        "else:\n",
        "    print(\"No agreement reached.\")"
      ],
      "metadata": {
        "id": "l8TM1kVLh2uQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# First-Price Auction Bidding Strategy"
      ],
      "metadata": {
        "id": "tAayqbawiBvw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def simulate_auction(n, valuations):\n",
        "    bids = []\n",
        "    for v in valuations:\n",
        "        bid = (1 - 1/n) * v\n",
        "        bids.append(bid)\n",
        "    return bids\n",
        "\n",
        "# Example usage\n",
        "n = 5  # Number of bidders\n",
        "valuations = np.random.uniform(0, 100, n)  # Generate random valuations for the bidders\n",
        "bids = simulate_auction(n, valuations)\n",
        "\n",
        "print(\"Valuations:\", valuations)\n",
        "print(\"Bids:\", bids)"
      ],
      "metadata": {
        "id": "h77ctZEbiCEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ultimatum Game Experimental Procedure"
      ],
      "metadata": {
        "id": "E_0xfq4diRx4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def ultimatum_game(num_rounds, total_amount):\n",
        "    results = []\n",
        "    for _ in range(num_rounds):\n",
        "        proposer = random.uniform(0, total_amount)\n",
        "        responder_acceptance_threshold = random.uniform(0, total_amount)\n",
        "\n",
        "        if proposer >= responder_acceptance_threshold:\n",
        "            results.append((proposer, total_amount - proposer))\n",
        "        else:\n",
        "            results.append((0, 0))\n",
        "    return results\n",
        "\n",
        "# Running the simulation\n",
        "num_rounds = 10  # Number of rounds/games to simulate\n",
        "total_amount = 100  # Total amount available for each game\n",
        "simulation_results = ultimatum_game(num_rounds, total_amount)\n",
        "print(simulation_results)"
      ],
      "metadata": {
        "id": "UfA-TDcZiSJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Greedy Algorithm for Influence Maximization"
      ],
      "metadata": {
        "id": "4Z6a1AoKic_J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def influence_maximization(graph, K):\n",
        "    V = list(graph.nodes())\n",
        "    S = set()\n",
        "\n",
        "    for k in range(1, K + 1):\n",
        "        max_increase = -np.inf\n",
        "        best_node = None\n",
        "\n",
        "        for v in V:\n",
        "            if v not in S:\n",
        "                S_plus_v = S | {v}\n",
        "                influence_increase = calculate_influence(graph, S_plus_v) - calculate_influence(graph, S)\n",
        "\n",
        "                if influence_increase > max_increase:\n",
        "                    max_increase = influence_increase\n",
        "                    best_node = v\n",
        "\n",
        "        S.add(best_node)\n",
        "\n",
        "    return S\n",
        "\n",
        "def calculate_influence(graph, seed_set):\n",
        "    # Simple influence calculation (for illustration)\n",
        "    return len(seed_set)\n",
        "\n",
        "# Example usage:\n",
        "import networkx as nx\n",
        "\n",
        "# Create a random graph (replace with your own graph if needed)\n",
        "G = nx.erdos_renyi_graph(30, 0.1, seed=42)\n",
        "\n",
        "# Run influence maximization with K=3\n",
        "seed_set = influence_maximization(G, K=3)\n",
        "print(\"Selected seed set:\", seed_set)"
      ],
      "metadata": {
        "id": "qupgERwRidVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Algorithm for Pairwise Stability Check"
      ],
      "metadata": {
        "id": "mYo_Boliinut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import networkx as nx\n",
        "\n",
        "def pairwise_stability_check(V, E, u, b, c):\n",
        "    \"\"\"\n",
        "    Perform pairwise stability check on a network.\n",
        "\n",
        "    Parameters:\n",
        "    - V: List of vertices (agents)\n",
        "    - E: List of edges (initial network structure)\n",
        "    - u: Dictionary of utilities for each agent u_i\n",
        "    - b: Dictionary of benefits for each potential link b_ij\n",
        "    - c: Dictionary of costs for each potential link c_ij\n",
        "\n",
        "    Returns:\n",
        "    - Updated network graph G after applying pairwise stability check\n",
        "    \"\"\"\n",
        "    G = nx.Graph()\n",
        "    G.add_nodes_from(V)\n",
        "    G.add_edges_from(E)\n",
        "\n",
        "    for i in range(len(V)):\n",
        "        for j in range(i+1, len(V)):\n",
        "            if (V[i], V[j]) in G.edges():\n",
        "                if u[V[i]] < u[V[i]] - c[(V[i], V[j])] or u[V[j]] < u[V[j]] - c[(V[i], V[j])]:\n",
        "                    G.remove_edge(V[i], V[j])\n",
        "            else:\n",
        "                if u[V[i]] + b[(V[i], V[j])] - c[(V[i], V[j])] > u[V[i]] or u[V[j]] + b[(V[i], V[j])] - c[(V[i], V[j])] > u[V[j]]:\n",
        "                    G.add_edge(V[i], V[j])\n",
        "\n",
        "    return G\n",
        "\n",
        "# Example usage:\n",
        "V = ['A', 'B', 'C']\n",
        "E = [('A', 'B')]\n",
        "u = {'A': 5, 'B': 4, 'C': 6}\n",
        "b = {('A', 'C'): 3, ('B', 'C'): 2}\n",
        "c = {('A', 'B'): 1, ('A', 'C'): 2, ('B', 'C'): 1}\n",
        "\n",
        "# Perform pairwise stability check\n",
        "updated_network = pairwise_stability_check(V, E, u, b, c)\n",
        "\n",
        "# Print the updated network edges\n",
        "print(\"Updated network edges:\", updated_network.edges())"
      ],
      "metadata": {
        "id": "vDcAy8ToioFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lemke-Howson Algorithm for Bimatrix Games"
      ],
      "metadata": {
        "id": "n6kmb9zwi25D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def lemke_howson(A, B):\n",
        "    \"\"\"\n",
        "    Implementation of Lemke-Howson algorithm for finding Nash equilibria in bimatrix games.\n",
        "\n",
        "    Parameters:\n",
        "    - A: Payoff matrix for Player 1\n",
        "    - B: Payoff matrix for Player 2\n",
        "\n",
        "    Returns:\n",
        "    - Mixed strategies for both players that constitute a Nash equilibrium\n",
        "    \"\"\"\n",
        "    m, n = A.shape\n",
        "    p = np.ones(m) / m  # Initial mixed strategy for Player 1\n",
        "    q = np.ones(n) / n  # Initial mixed strategy for Player 2\n",
        "    basis = [-1] * (m + n)  # Basis vector to track entering and leaving variables\n",
        "    entering = 0  # Index of the entering variable\n",
        "\n",
        "    # Artificial equilibrium to start the algorithm\n",
        "    eq = np.hstack((p, q))\n",
        "    eq[m + n - 1] = 1\n",
        "\n",
        "    # Lemke-Howson algorithm loop\n",
        "    while True:\n",
        "        # Check if current solution is a Nash equilibrium\n",
        "        if np.allclose(np.dot(A.T, p), B @ q, atol=1e-8) and np.allclose(np.dot(p, A), np.dot(B, q), atol=1e-8):\n",
        "            break\n",
        "\n",
        "        # Find the entering variable\n",
        "        for i in range(m + n):\n",
        "            if eq[i] == 0:\n",
        "                entering = i\n",
        "                break\n",
        "\n",
        "        # Pivot to update strategies\n",
        "        if entering < m:\n",
        "            p = np.zeros(m)\n",
        "            basis[entering] = 1\n",
        "            index = entering\n",
        "        else:\n",
        "            q = np.zeros(n)\n",
        "            basis[entering] = 0\n",
        "            index = entering - m\n",
        "\n",
        "        # Follow the path along the edges of the polyhedron\n",
        "        count = 0\n",
        "        while True:\n",
        "            count += 1\n",
        "            if count % 2 == 1:\n",
        "                q = B.T @ p\n",
        "                if q[index] > 0:\n",
        "                    entering = np.argmax(q)\n",
        "                    basis[m + entering] = 1\n",
        "                else:\n",
        "                    break\n",
        "            else:\n",
        "                p = A @ q\n",
        "                if p[index] > 0:\n",
        "                    entering = np.argmax(p)\n",
        "                    basis[entering] = 0\n",
        "                else:\n",
        "                    break\n",
        "\n",
        "        # Update mixed strategies according to best responses\n",
        "        for i in range(m + n):\n",
        "            if basis[i] == -1:\n",
        "                continue\n",
        "            elif basis[i] < m:\n",
        "                p[basis[i]] = eq[i]\n",
        "            else:\n",
        "                q[basis[i] - m] = eq[i]\n",
        "\n",
        "        # Normalize strategies\n",
        "        p /= np.sum(p)\n",
        "        q /= np.sum(q)\n",
        "\n",
        "        # Update equilibrium solution\n",
        "        eq = np.hstack((p, q))\n",
        "\n",
        "    return p, q\n",
        "\n",
        "# Example usage:\n",
        "A = np.array([[2, 0], [1, 3]])\n",
        "B = np.array([[1, 0], [0, 2]])\n",
        "\n",
        "p, q = lemke_howson(A, B)\n",
        "print(\"Player 1's mixed strategy:\", p)\n",
        "print(\"Player 2's mixed strategy:\", q)"
      ],
      "metadata": {
        "id": "mAPmoof2i3UY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fictitious Play Algorithm"
      ],
      "metadata": {
        "id": "vrNq1QkcjBM2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def fictitious_play(payoff_matrix, iterations):\n",
        "    n, m = payoff_matrix.shape[0], payoff_matrix.shape[1]\n",
        "    p1 = np.ones(n) / n\n",
        "    p2 = np.ones(m) / m\n",
        "    history1 = np.zeros(n)\n",
        "    history2 = np.zeros(m)\n",
        "\n",
        "    for t in range(iterations):\n",
        "        action1 = np.random.choice(np.arange(n), p=p1)  # Player 1's action selection\n",
        "        action2 = np.random.choice(np.arange(m), p=p2)  # Player 2's action selection\n",
        "\n",
        "        history1[action1] += 1\n",
        "        history2[action2] += 1\n",
        "\n",
        "        p1 = history1 / np.sum(history1)\n",
        "        p2 = history2 / np.sum(history2)\n",
        "\n",
        "    return p1, p2\n",
        "\n",
        "# Example usage:\n",
        "payoff_matrix = np.array([[4, 0], [2, 3]])\n",
        "iterations = 1000\n",
        "p1, p2 = fictitious_play(payoff_matrix, iterations)\n",
        "print(f\"Player 1 strategy: {p1}\")\n",
        "print(f\"Player 2 strategy: {p2}\")"
      ],
      "metadata": {
        "id": "ULTaDj1jjBkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q-Learning for Optimal Strategy"
      ],
      "metadata": {
        "id": "ZykzQyFxjQJW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def q_learning(env, num_episodes, alpha, gamma, epsilon):\n",
        "    # Initialize Q-table arbitrarily\n",
        "    num_states = env.observation_space.n\n",
        "    num_actions = env.action_space.n\n",
        "    Q = np.zeros((num_states, num_actions))\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            # Choose action using epsilon-greedy policy\n",
        "            if np.random.uniform(0, 1) < epsilon:\n",
        "                action = env.action_space.sample()  # Explore\n",
        "            else:\n",
        "                action = np.argmax(Q[state])  # Exploit\n",
        "\n",
        "            # Take action and observe next state and reward\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "            # Update Q-value using Q-learning update rule\n",
        "            Q[state, action] = Q[state, action] + alpha * (reward + gamma * np.max(Q[next_state]) - Q[state, action])\n",
        "\n",
        "            # Move to next state\n",
        "            state = next_state\n",
        "\n",
        "    return Q\n",
        "\n",
        "# Example usage (using OpenAI Gym's FrozenLake environment)\n",
        "import gym\n",
        "\n",
        "env = gym.make('FrozenLake-v0')\n",
        "num_episodes = 10000\n",
        "alpha = 0.1\n",
        "gamma = 0.99\n",
        "epsilon = 0.1\n",
        "\n",
        "Q = q_learning(env, num_episodes, alpha, gamma, epsilon)\n",
        "print(\"Q-table:\")\n",
        "print(Q)"
      ],
      "metadata": {
        "id": "8kjOXsr4jQiT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}