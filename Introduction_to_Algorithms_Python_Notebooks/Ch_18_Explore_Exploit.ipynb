{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Upper Confidence Bound (UCB) Algorithm"
      ],
      "metadata": {
        "id": "BEr_uqcWxGkO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mUARL2Z8w-h6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class UCB:\n",
        "    def __init__(self, n_arms):\n",
        "        self.n_arms = n_arms\n",
        "        self.counts = np.zeros(n_arms)  # Count of pulls for each arm\n",
        "        self.values = np.zeros(n_arms)  # Average reward for each arm\n",
        "\n",
        "    def select_arm(self):\n",
        "        total_counts = np.sum(self.counts)\n",
        "        if total_counts < self.n_arms:\n",
        "            # Ensure each arm is selected at least once initially\n",
        "            return int(total_counts)\n",
        "\n",
        "        ucb_values = self.values + np.sqrt((2 * np.log(total_counts)) / self.counts)\n",
        "        return np.argmax(ucb_values)\n",
        "\n",
        "    def update(self, chosen_arm, reward):\n",
        "        self.counts[chosen_arm] += 1\n",
        "        n = self.counts[chosen_arm]\n",
        "        value = self.values[chosen_arm]\n",
        "        new_value = ((n - 1) / n) * value + (1 / n) * reward\n",
        "        self.values[chosen_arm] = new_value\n",
        "\n",
        "def ucb_simulation(n_arms, n_rounds, true_means):\n",
        "    ucb = UCB(n_arms)\n",
        "    rewards = np.zeros(n_rounds)\n",
        "\n",
        "    for round in range(n_rounds):\n",
        "        chosen_arm = ucb.select_arm()\n",
        "        reward = np.random.randn() + true_means[chosen_arm]\n",
        "        ucb.update(chosen_arm, reward)\n",
        "        rewards[round] = reward\n",
        "\n",
        "    return rewards, ucb\n",
        "\n",
        "# Example usage\n",
        "n_arms = 5  # Number of arms\n",
        "n_rounds = 1000  # Number of rounds\n",
        "true_means = [0.1, 0.2, 0.3, 0.4, 0.5]  # True means of the arms\n",
        "\n",
        "rewards, ucb = ucb_simulation(n_arms, n_rounds, true_means)\n",
        "print(\"Average Reward:\", np.mean(rewards))\n",
        "print(\"Counts of each arm:\", ucb.counts)\n",
        "print(\"Estimated values of each arm:\", ucb.values)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Epsilon-Greedy Algorithm"
      ],
      "metadata": {
        "id": "gtLutzCUxbyk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class EpsilonGreedy:\n",
        "    def __init__(self, n_arms, epsilon):\n",
        "        self.n_arms = n_arms\n",
        "        self.epsilon = epsilon\n",
        "        self.counts = np.zeros(n_arms)  # Count of pulls for each arm\n",
        "        self.values = np.zeros(n_arms)  # Average reward for each arm\n",
        "\n",
        "    def select_arm(self):\n",
        "        if np.random.rand() < self.epsilon:\n",
        "            return np.random.randint(0, self.n_arms)  # Explore\n",
        "        else:\n",
        "            return np.argmax(self.values)  # Exploit\n",
        "\n",
        "    def update(self, chosen_arm, reward):\n",
        "        self.counts[chosen_arm] += 1\n",
        "        n = self.counts[chosen_arm]\n",
        "        value = self.values[chosen_arm]\n",
        "        new_value = ((n - 1) / n) * value + (1 / n) * reward\n",
        "        self.values[chosen_arm] = new_value\n",
        "\n",
        "def epsilon_greedy_simulation(n_arms, n_rounds, true_means, epsilon):\n",
        "    epsilon_greedy = EpsilonGreedy(n_arms, epsilon)\n",
        "    rewards = np.zeros(n_rounds)\n",
        "\n",
        "    for round in range(n_rounds):\n",
        "        chosen_arm = epsilon_greedy.select_arm()\n",
        "        reward = np.random.randn() + true_means[chosen_arm]\n",
        "        epsilon_greedy.update(chosen_arm, reward)\n",
        "        rewards[round] = reward\n",
        "\n",
        "    return rewards, epsilon_greedy\n",
        "\n",
        "# Example usage\n",
        "n_arms = 5  # Number of arms\n",
        "n_rounds = 1000  # Number of rounds\n",
        "true_means = [0.1, 0.2, 0.3, 0.4, 0.5]  # True means of the arms\n",
        "epsilon = 0.1  # Epsilon value\n",
        "\n",
        "rewards, epsilon_greedy = epsilon_greedy_simulation(n_arms, n_rounds, true_means, epsilon)\n",
        "print(\"Average Reward:\", np.mean(rewards))\n",
        "print(\"Counts of each arm:\", epsilon_greedy.counts)\n",
        "print(\"Estimated values of each arm:\", epsilon_greedy.values)"
      ],
      "metadata": {
        "id": "JKknYZsKxcT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Thompson Sampling Algorithm"
      ],
      "metadata": {
        "id": "QuSbx8FXxkS3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class ThompsonSampling:\n",
        "    def __init__(self, n_arms):\n",
        "        self.n_arms = n_arms\n",
        "        self.successes = np.zeros(n_arms)  # Number of successes for each arm\n",
        "        self.failures = np.zeros(n_arms)   # Number of failures for each arm\n",
        "\n",
        "    def select_arm(self):\n",
        "        samples = np.zeros(self.n_arms)\n",
        "        for arm in range(self.n_arms):\n",
        "            samples[arm] = np.random.beta(self.successes[arm] + 1, self.failures[arm] + 1)\n",
        "        return np.argmax(samples)\n",
        "\n",
        "    def update(self, chosen_arm, reward):\n",
        "        if reward == 1:\n",
        "            self.successes[chosen_arm] += 1\n",
        "        else:\n",
        "            self.failures[chosen_arm] += 1\n",
        "\n",
        "def thompson_sampling_simulation(n_arms, n_rounds, true_means):\n",
        "    thompson_sampling = ThompsonSampling(n_arms)\n",
        "    rewards = np.zeros(n_rounds)\n",
        "\n",
        "    for round in range(n_rounds):\n",
        "        chosen_arm = thompson_sampling.select_arm()\n",
        "        reward = np.random.rand() < true_means[chosen_arm]  # Bernoulli reward\n",
        "        thompson_sampling.update(chosen_arm, reward)\n",
        "        rewards[round] = reward\n",
        "\n",
        "    return rewards, thompson_sampling\n",
        "\n",
        "# Example usage\n",
        "n_arms = 5  # Number of arms\n",
        "n_rounds = 1000  # Number of rounds\n",
        "true_means = [0.1, 0.2, 0.3, 0.4, 0.5]  # True means of the arms\n",
        "\n",
        "rewards, thompson_sampling = thompson_sampling_simulation(n_arms, n_rounds, true_means)\n",
        "print(\"Average Reward:\", np.mean(rewards))\n",
        "print(\"Successes of each arm:\", thompson_sampling.successes)\n",
        "print(\"Failures of each arm:\", thompson_sampling.failures)"
      ],
      "metadata": {
        "id": "zUgrVI50xkpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q-learning Algorithm for Reinforcement Learning"
      ],
      "metadata": {
        "id": "jqkts4v7xy17"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "class QLearning:\n",
        "    def __init__(self, n_states, n_actions, alpha=0.1, gamma=0.99, epsilon=0.1):\n",
        "        self.n_states = n_states\n",
        "        self.n_actions = n_actions\n",
        "        self.alpha = alpha  # Learning rate\n",
        "        self.gamma = gamma  # Discount factor\n",
        "        self.epsilon = epsilon  # Exploration rate\n",
        "        self.q_table = np.zeros((n_states, n_actions))  # Initialize Q-table\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        if random.uniform(0, 1) < self.epsilon:\n",
        "            return random.randint(0, self.n_actions - 1)  # Explore: random action\n",
        "        else:\n",
        "            return np.argmax(self.q_table[state, :])  # Exploit: best action\n",
        "\n",
        "    def update_q_table(self, state, action, reward, next_state):\n",
        "        best_next_action = np.argmax(self.q_table[next_state, :])\n",
        "        td_target = reward + self.gamma * self.q_table[next_state, best_next_action]\n",
        "        td_error = td_target - self.q_table[state, action]\n",
        "        self.q_table[state, action] += self.alpha * td_error\n",
        "\n",
        "def train_q_learning(env, q_learning, n_episodes):\n",
        "    rewards = []\n",
        "    for episode in range(n_episodes):\n",
        "        state = env.reset()\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "        while not done:\n",
        "            action = q_learning.choose_action(state)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            q_learning.update_q_table(state, action, reward, next_state)\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "        rewards.append(total_reward)\n",
        "    return rewards\n",
        "\n",
        "# Example Grid World Environment\n",
        "class SimpleGridWorld:\n",
        "    def __init__(self, size=5):\n",
        "        self.size = size\n",
        "        self.n_states = size * size\n",
        "        self.n_actions = 4  # Up, Down, Left, Right\n",
        "        self.goal_state = self.n_states - 1\n",
        "        self.state = 0\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = 0\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        row, col = divmod(self.state, self.size)\n",
        "        if action == 0:  # Up\n",
        "            row = max(row - 1, 0)\n",
        "        elif action == 1:  # Down\n",
        "            row = min(row + 1, self.size - 1)\n",
        "        elif action == 2:  # Left\n",
        "            col = max(col - 1, 0)\n",
        "        elif action == 3:  # Right\n",
        "            col = min(col + 1, self.size - 1)\n",
        "\n",
        "        self.state = row * self.size + col\n",
        "        reward = 1 if self.state == self.goal_state else -0.01\n",
        "        done = self.state == self.goal_state\n",
        "        return self.state, reward, done, {}\n",
        "\n",
        "# Example usage\n",
        "env = SimpleGridWorld(size=5)\n",
        "q_learning = QLearning(n_states=env.n_states, n_actions=env.n_actions, alpha=0.1, gamma=0.99, epsilon=0.1)\n",
        "n_episodes = 1000\n",
        "rewards = train_q_learning(env, q_learning, n_episodes)\n",
        "\n",
        "print(\"Trained Q-table:\")\n",
        "print(q_learning.q_table)\n",
        "print(\"Rewards over episodes:\")\n",
        "print(rewards)"
      ],
      "metadata": {
        "id": "BMM3ZBylxzMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Epsilon-Greedy strategy"
      ],
      "metadata": {
        "id": "eYCejttwyFmI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def epsilon_greedy(num_actions, epsilon, num_steps):\n",
        "    \"\"\"\n",
        "    Implements the epsilon-greedy algorithm for multi-armed bandit problem.\n",
        "\n",
        "    Args:\n",
        "        num_actions (int): Number of actions available.\n",
        "        epsilon (float): Probability of selecting a random action.\n",
        "        num_steps (int): Number of steps to run the algorithm.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: Action-value estimates Q(a) for each action a.\n",
        "    \"\"\"\n",
        "    # Initialize action-value estimates Q(a) for each action a\n",
        "    Q = np.zeros(num_actions)\n",
        "    # Initialize counts of each action N(a) = 0 for each action a\n",
        "    N = np.zeros(num_actions)\n",
        "\n",
        "    for t in range(1, num_steps + 1):\n",
        "        # With probability epsilon, select a random action\n",
        "        if np.random.random() < epsilon:\n",
        "            action = np.random.randint(num_actions)\n",
        "        # Otherwise, select argmax_a Q(a)\n",
        "        else:\n",
        "            action = np.argmax(Q)\n",
        "\n",
        "        # Simulate taking the selected action and observe reward\n",
        "        reward = simulate_environment(action)\n",
        "\n",
        "        # Update action-value estimate Q(a)\n",
        "        N[action] += 1\n",
        "        Q[action] += (1 / N[action]) * (reward - Q[action])\n",
        "\n",
        "    return Q\n",
        "\n",
        "def simulate_environment(action):\n",
        "    \"\"\"\n",
        "    Simulates the environment and returns the reward for the selected action.\n",
        "\n",
        "    Args:\n",
        "        action (int): The action selected.\n",
        "\n",
        "    Returns:\n",
        "        float: The reward received for the selected action.\n",
        "    \"\"\"\n",
        "    # Simulate environment and return reward for selected action\n",
        "    return np.random.normal(loc=action, scale=1)\n",
        "\n",
        "# Example usage\n",
        "num_actions = 10  # Number of actions\n",
        "epsilon = 0.1     # Probability of selecting a random action\n",
        "num_steps = 1000  # Number of steps to run the algorithm\n",
        "\n",
        "# Run epsilon-greedy algorithm\n",
        "action_value_estimates = epsilon_greedy(num_actions, epsilon, num_steps)\n",
        "print(\"Action-value estimates:\", action_value_estimates)"
      ],
      "metadata": {
        "id": "4DfEt5jhyGDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explore-Exploit for Dynamic Pricing"
      ],
      "metadata": {
        "id": "Qi-MFQKl0SEg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def dynamic_pricing(mu, sigma, time_horizon):\n",
        "    \"\"\"\n",
        "    Implements a dynamic pricing strategy over a given time horizon.\n",
        "\n",
        "    Args:\n",
        "        mu (float): Mean demand.\n",
        "        sigma (float): Standard deviation of demand.\n",
        "        time_horizon (int): The total time horizon for the pricing strategy.\n",
        "\n",
        "    Returns:\n",
        "        float: The total revenue generated over the time horizon.\n",
        "    \"\"\"\n",
        "    revenue = 0\n",
        "    exploration_period = int(time_horizon * 0.1)  # Define exploration period (e.g., 10% of time horizon)\n",
        "\n",
        "    for t in range(time_horizon):\n",
        "        # Simulate demand using a normal distribution\n",
        "        demand = np.random.normal(mu, sigma)\n",
        "\n",
        "        # Choose pricing strategy based on exploration/exploitation phase\n",
        "        if t < exploration_period:\n",
        "            price = explore_pricing()\n",
        "        else:\n",
        "            price = exploit_pricing()\n",
        "\n",
        "        # Calculate revenue for the current time step\n",
        "        revenue += price * demand\n",
        "\n",
        "    return revenue\n",
        "\n",
        "def explore_pricing():\n",
        "    \"\"\"\n",
        "    Exploration phase pricing strategy.\n",
        "\n",
        "    Returns:\n",
        "        float: The price during the exploration phase.\n",
        "    \"\"\"\n",
        "    # Example: Return a random price for exploration\n",
        "    return np.random.uniform(50, 150)\n",
        "\n",
        "def exploit_pricing():\n",
        "    \"\"\"\n",
        "    Exploitation phase pricing strategy.\n",
        "\n",
        "    Returns:\n",
        "        float: The price during the exploitation phase.\n",
        "    \"\"\"\n",
        "    # Example: Return an optimal price based on some logic or model\n",
        "    return 100  # Replace with a more sophisticated pricing strategy\n",
        "\n",
        "# Example usage\n",
        "mu = 100  # Mean demand\n",
        "sigma = 20  # Standard deviation of demand\n",
        "time_horizon = 1000  # Total time horizon\n",
        "\n",
        "# Run dynamic pricing algorithm\n",
        "revenue = dynamic_pricing(mu, sigma, time_horizon)\n",
        "print(\"Total revenue:\", revenue)"
      ],
      "metadata": {
        "id": "HoMlMrDO0VOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explore-Exploit for Revenue Management"
      ],
      "metadata": {
        "id": "nwklW6zp0eAs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def revenue_management(time_horizon):\n",
        "    \"\"\"\n",
        "    Implements a revenue management strategy over a given time horizon.\n",
        "\n",
        "    Args:\n",
        "        time_horizon (int): The total time horizon for the revenue management strategy.\n",
        "\n",
        "    Returns:\n",
        "        float: The total revenue generated over the time horizon.\n",
        "    \"\"\"\n",
        "    revenue = 0\n",
        "    exploration_period = int(time_horizon * 0.1)  # Define exploration period (e.g., 10% of time horizon)\n",
        "\n",
        "    for t in range(time_horizon):\n",
        "        # Simulate observing the demand\n",
        "        demand = observe_demand()\n",
        "\n",
        "        # Choose strategy based on exploration/exploitation phase\n",
        "        if t < exploration_period:\n",
        "            strategy = explore_strategy()\n",
        "        else:\n",
        "            strategy = exploit_strategy()\n",
        "\n",
        "        # Allocate resources based on the chosen strategy and calculate revenue\n",
        "        revenue += allocate_resources(strategy) * demand\n",
        "\n",
        "    return revenue\n",
        "\n",
        "def observe_demand():\n",
        "    \"\"\"\n",
        "    Simulates observing demand.\n",
        "\n",
        "    Returns:\n",
        "        float: The observed demand value.\n",
        "    \"\"\"\n",
        "    # Example: Return a random demand value (this can be replaced with actual demand observation logic)\n",
        "    return random.uniform(50, 150)\n",
        "\n",
        "def explore_strategy():\n",
        "    \"\"\"\n",
        "    Exploration phase strategy.\n",
        "\n",
        "    Returns:\n",
        "        str: The strategy during the exploration phase.\n",
        "    \"\"\"\n",
        "    # Example: Return a random strategy for exploration (this can be more sophisticated)\n",
        "    return random.choice([\"strategy1\", \"strategy2\", \"strategy3\"])\n",
        "\n",
        "def exploit_strategy():\n",
        "    \"\"\"\n",
        "    Exploitation phase strategy.\n",
        "\n",
        "    Returns:\n",
        "        str: The strategy during the exploitation phase.\n",
        "    \"\"\"\n",
        "    # Example: Return an optimal strategy based on some logic or model\n",
        "    return \"optimal_strategy\"\n",
        "\n",
        "def allocate_resources(strategy):\n",
        "    \"\"\"\n",
        "    Allocates resources based on the chosen strategy.\n",
        "\n",
        "    Args:\n",
        "        strategy (str): The chosen strategy.\n",
        "\n",
        "    Returns:\n",
        "        float: The allocation factor based on the strategy.\n",
        "    \"\"\"\n",
        "    # Example: Return an allocation factor based on the strategy (this can be more sophisticated)\n",
        "    strategy_allocation = {\n",
        "        \"strategy1\": 1.1,\n",
        "        \"strategy2\": 1.2,\n",
        "        \"strategy3\": 1.3,\n",
        "        \"optimal_strategy\": 1.5\n",
        "    }\n",
        "    return strategy_allocation.get(strategy, 1.0)\n",
        "\n",
        "# Example usage\n",
        "time_horizon = 100  # Total time horizon\n",
        "\n",
        "# Run revenue management algorithm\n",
        "total_revenue = revenue_management(time_horizon)\n",
        "print(\"Total revenue:\", total_revenue)"
      ],
      "metadata": {
        "id": "Tiz88Eh00eYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explore-Exploit for Content Recommendation"
      ],
      "metadata": {
        "id": "7IioBrCM0r2W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def content_recommendation(user_preferences):\n",
        "    \"\"\"\n",
        "    Content recommendation system that alternates between exploration and exploitation phases.\n",
        "\n",
        "    Args:\n",
        "        user_preferences (dict): User preferences for content recommendation.\n",
        "\n",
        "    Returns:\n",
        "        list: List of recommended items.\n",
        "    \"\"\"\n",
        "    recommendation_strategy = initialize_strategy()\n",
        "    recommended_items = []\n",
        "\n",
        "    for interaction in user_interactions:\n",
        "        # Recommend items based on the current strategy\n",
        "        recommended_items = recommend_items(recommendation_strategy)\n",
        "\n",
        "        # Observe user feedback from the interaction\n",
        "        observe_feedback(interaction)\n",
        "\n",
        "        # Update strategy based on the phase (exploration or exploitation)\n",
        "        if exploration_phase():\n",
        "            update_strategy_for_exploration()\n",
        "        else:\n",
        "            update_strategy_for_exploitation(user_preferences)\n",
        "\n",
        "    return recommended_items\n",
        "\n",
        "def initialize_strategy():\n",
        "    \"\"\"\n",
        "    Initializes the recommendation strategy.\n",
        "\n",
        "    Returns:\n",
        "        str: Initial recommendation strategy.\n",
        "    \"\"\"\n",
        "    # Example: Return a basic initial strategy\n",
        "    return \"initial_strategy\"\n",
        "\n",
        "def recommend_items(strategy):\n",
        "    \"\"\"\n",
        "    Recommends items based on the current strategy.\n",
        "\n",
        "    Args:\n",
        "        strategy (str): The current recommendation strategy.\n",
        "\n",
        "    Returns:\n",
        "        list: List of recommended items.\n",
        "    \"\"\"\n",
        "    # Example: Return a list of recommended items based on the strategy\n",
        "    if strategy == \"initial_strategy\":\n",
        "        return [\"item1\", \"item2\", \"item3\"]\n",
        "    elif strategy == \"exploration_strategy\":\n",
        "        return [\"item4\", \"item5\", \"item6\"]\n",
        "    elif strategy == \"exploitation_strategy\":\n",
        "        return [\"item7\", \"item8\", \"item9\"]\n",
        "\n",
        "def observe_feedback(interaction):\n",
        "    \"\"\"\n",
        "    Observes user feedback from an interaction.\n",
        "\n",
        "    Args:\n",
        "        interaction (dict): The interaction data.\n",
        "    \"\"\"\n",
        "    # Example: Process the interaction feedback (this is a placeholder)\n",
        "    print(f\"Observed feedback from interaction: {interaction}\")\n",
        "\n",
        "def exploration_phase():\n",
        "    \"\"\"\n",
        "    Determines whether the system is in the exploration phase.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if in exploration phase, False otherwise.\n",
        "    \"\"\"\n",
        "    # Example: Return True if in exploration phase, False otherwise (this can be more sophisticated)\n",
        "    return random.choice([True, False])\n",
        "\n",
        "def update_strategy_for_exploration():\n",
        "    \"\"\"\n",
        "    Updates the recommendation strategy for the exploration phase.\n",
        "    \"\"\"\n",
        "    # Example: Update the strategy for exploration phase (this is a placeholder)\n",
        "    print(\"Updating strategy for exploration phase\")\n",
        "\n",
        "def update_strategy_for_exploitation(user_preferences):\n",
        "    \"\"\"\n",
        "    Updates the recommendation strategy for the exploitation phase based on user preferences.\n",
        "\n",
        "    Args:\n",
        "        user_preferences (dict): User preferences for content recommendation.\n",
        "    \"\"\"\n",
        "    # Example: Update the strategy for exploitation phase based on user preferences (this is a placeholder)\n",
        "    print(f\"Updating strategy for exploitation phase based on user preferences: {user_preferences}\")\n",
        "\n",
        "def initialize_user_preferences():\n",
        "    \"\"\"\n",
        "    Initializes user preferences.\n",
        "\n",
        "    Returns:\n",
        "        dict: Initialized user preferences.\n",
        "    \"\"\"\n",
        "    # Example: Return a dictionary of user preferences (this is a placeholder)\n",
        "    return {\"genre\": \"sci-fi\", \"length\": \"short\", \"format\": \"video\"}\n",
        "\n",
        "# Example usage\n",
        "user_interactions = [\n",
        "    {\"item\": \"item1\", \"feedback\": \"liked\"},\n",
        "    {\"item\": \"item2\", \"feedback\": \"disliked\"},\n",
        "    {\"item\": \"item3\", \"feedback\": \"liked\"},\n",
        "    # More interactions...\n",
        "]\n",
        "\n",
        "# Initialize user preferences\n",
        "user_preferences = initialize_user_preferences()\n",
        "\n",
        "# Run content recommendation algorithm\n",
        "recommended_items = content_recommendation(user_preferences)\n",
        "print(\"Recommended items:\", recommended_items)"
      ],
      "metadata": {
        "id": "Se3VKrbY0sO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explore-Exploit for Adaptive Routing"
      ],
      "metadata": {
        "id": "QdUKhbfO04aS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def adaptive_routing(network_state):\n",
        "    \"\"\"\n",
        "    Adaptive routing algorithm that alternates between exploration and exploitation phases.\n",
        "\n",
        "    Args:\n",
        "        network_state (dict): Current state of the network.\n",
        "\n",
        "    Returns:\n",
        "        list: Routed traffic information.\n",
        "    \"\"\"\n",
        "    routing_policies = initialize_policies()\n",
        "    routed_traffic = []\n",
        "\n",
        "    for incoming_packet in network_traffic:\n",
        "        # Route packet based on the current routing policies\n",
        "        route_packet(incoming_packet, routing_policies)\n",
        "\n",
        "        # Monitor network performance\n",
        "        monitor_network_performance(network_state)\n",
        "\n",
        "        # Update routing policies based on the phase (exploration or exploitation)\n",
        "        if exploration_phase():\n",
        "            explore_alternative_paths()\n",
        "        else:\n",
        "            exploit_high_performing_paths()\n",
        "\n",
        "        routed_traffic.append(incoming_packet)\n",
        "\n",
        "    return routed_traffic\n",
        "\n",
        "def initialize_policies():\n",
        "    \"\"\"\n",
        "    Initializes routing policies.\n",
        "\n",
        "    Returns:\n",
        "        dict: Initial routing policies.\n",
        "    \"\"\"\n",
        "    # Example: Return a basic initial policy\n",
        "    return {\"policy\": \"initial_policy\"}\n",
        "\n",
        "def route_packet(packet, policies):\n",
        "    \"\"\"\n",
        "    Routes an incoming packet based on the current routing policies.\n",
        "\n",
        "    Args:\n",
        "        packet (dict): Incoming packet information.\n",
        "        policies (dict): Current routing policies.\n",
        "    \"\"\"\n",
        "    # Example: Route the packet based on the policies (this is a placeholder)\n",
        "    print(f\"Routing packet {packet} using policies {policies}\")\n",
        "\n",
        "def monitor_network_performance(network_state):\n",
        "    \"\"\"\n",
        "    Monitors the performance of the network.\n",
        "\n",
        "    Args:\n",
        "        network_state (dict): Current state of the network.\n",
        "    \"\"\"\n",
        "    # Example: Monitor the network performance (this is a placeholder)\n",
        "    print(f\"Monitoring network performance: {network_state}\")\n",
        "\n",
        "def exploration_phase():\n",
        "    \"\"\"\n",
        "    Determines whether the system is in the exploration phase.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if in exploration phase, False otherwise.\n",
        "    \"\"\"\n",
        "    # Example: Return True if in exploration phase, False otherwise (this can be more sophisticated)\n",
        "    return random.choice([True, False])\n",
        "\n",
        "def explore_alternative_paths():\n",
        "    \"\"\"\n",
        "    Explores alternative paths for routing.\n",
        "    \"\"\"\n",
        "    # Example: Explore alternative paths (this is a placeholder)\n",
        "    print(\"Exploring alternative paths\")\n",
        "\n",
        "def exploit_high_performing_paths():\n",
        "    \"\"\"\n",
        "    Exploits high-performing paths for routing.\n",
        "    \"\"\"\n",
        "    # Example: Exploit high-performing paths (this is a placeholder)\n",
        "    print(\"Exploiting high-performing paths\")\n",
        "\n",
        "def initialize_network_state():\n",
        "    \"\"\"\n",
        "    Initializes the network state.\n",
        "\n",
        "    Returns:\n",
        "        dict: Initial network state.\n",
        "    \"\"\"\n",
        "    # Example: Return a dictionary representing the network state (this is a placeholder)\n",
        "    return {\"state\": \"initial_state\"}\n",
        "\n",
        "# Example usage\n",
        "network_traffic = [\n",
        "    {\"packet_id\": 1, \"source\": \"A\", \"destination\": \"B\"},\n",
        "    {\"packet_id\": 2, \"source\": \"A\", \"destination\": \"C\"},\n",
        "    {\"packet_id\": 3, \"source\": \"B\", \"destination\": \"C\"},\n",
        "    # More packets...\n",
        "]\n",
        "\n",
        "# Initialize network state\n",
        "network_state = initialize_network_state()\n",
        "\n",
        "# Run adaptive routing algorithm\n",
        "routed_traffic = adaptive_routing(network_state)\n",
        "print(\"Routed traffic:\", routed_traffic)"
      ],
      "metadata": {
        "id": "1awnD1jX041v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bayesian Optimal Interval for Dose-Finding"
      ],
      "metadata": {
        "id": "fwv__VuF1ZXn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class BayesianOptimalInterval:\n",
        "    \"\"\"\n",
        "    Bayesian Optimal Interval class to manage dose selection and update posterior distribution\n",
        "    based on observed responses.\n",
        "    \"\"\"\n",
        "    def __init__(self, prior_mean, prior_std):\n",
        "        \"\"\"\n",
        "        Initialize the BayesianOptimalInterval with prior mean and standard deviation.\n",
        "\n",
        "        Args:\n",
        "            prior_mean (float): Prior mean of the distribution.\n",
        "            prior_std (float): Prior standard deviation of the distribution.\n",
        "        \"\"\"\n",
        "        self.prior_mean = prior_mean\n",
        "        self.prior_std = prior_std\n",
        "        self.posterior_mean = prior_mean\n",
        "        self.posterior_std = prior_std\n",
        "\n",
        "    def select_dose(self):\n",
        "        \"\"\"\n",
        "        Select the dose based on the posterior mean.\n",
        "\n",
        "        Returns:\n",
        "            float: The selected dose.\n",
        "        \"\"\"\n",
        "        return self.posterior_mean\n",
        "\n",
        "    def update_distribution(self, dose, response):\n",
        "        \"\"\"\n",
        "        Update the posterior distribution based on the observed response.\n",
        "\n",
        "        Args:\n",
        "            dose (float): The dose that was administered.\n",
        "            response (float): The observed response.\n",
        "        \"\"\"\n",
        "        self.posterior_mean = (self.prior_mean + response) / 2\n",
        "        self.posterior_std = max(self.prior_std / 2, 0.1)\n",
        "\n",
        "    def compute_optimal_interval(self):\n",
        "        \"\"\"\n",
        "        Compute the optimal interval based on the posterior distribution.\n",
        "        This function is a placeholder and should be implemented based on the specific requirements.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "# Example usage:\n",
        "boin = BayesianOptimalInterval(prior_mean=0, prior_std=1)\n",
        "dose = boin.select_dose()\n",
        "response = 1  # Assume patient response is observed\n",
        "boin.update_distribution(dose, response)\n",
        "print(f\"Updated Posterior Mean: {boin.posterior_mean}\")\n",
        "print(f\"Updated Posterior Standard Deviation: {boin.posterior_std}\")"
      ],
      "metadata": {
        "id": "FY-GRmfs1Zta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q-Learning"
      ],
      "metadata": {
        "id": "dUxa5cAf1l2X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class QLearning:\n",
        "    \"\"\"\n",
        "    Q-Learning algorithm class for reinforcement learning.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_states, num_actions):\n",
        "        \"\"\"\n",
        "        Initialize Q-Learning with the given number of states and actions.\n",
        "\n",
        "        Args:\n",
        "            num_states (int): Number of states in the environment.\n",
        "            num_actions (int): Number of possible actions.\n",
        "        \"\"\"\n",
        "        self.Q = np.zeros((num_states, num_actions))  # Initialize Q-table with zeros\n",
        "\n",
        "    def select_action(self, state, epsilon):\n",
        "        \"\"\"\n",
        "        Select an action based on the epsilon-greedy policy.\n",
        "\n",
        "        Args:\n",
        "            state (int): The current state.\n",
        "            epsilon (float): Probability of selecting a random action (exploration).\n",
        "\n",
        "        Returns:\n",
        "            int: The selected action.\n",
        "        \"\"\"\n",
        "        if np.random.rand() < epsilon:\n",
        "            return np.random.choice(self.Q.shape[1])  # Explore: select a random action\n",
        "        else:\n",
        "            return np.argmax(self.Q[state])  # Exploit: select the action with max Q-value\n",
        "\n",
        "    def update_q_value(self, state, action, reward, next_state, alpha, gamma):\n",
        "        \"\"\"\n",
        "        Update the Q-value for the given state-action pair.\n",
        "\n",
        "        Args:\n",
        "            state (int): The current state.\n",
        "            action (int): The action taken.\n",
        "            reward (float): The reward received after taking the action.\n",
        "            next_state (int): The next state after taking the action.\n",
        "            alpha (float): Learning rate.\n",
        "            gamma (float): Discount factor.\n",
        "        \"\"\"\n",
        "        max_next_q = np.max(self.Q[next_state])  # Max Q-value for the next state\n",
        "        self.Q[state, action] += alpha * (reward + gamma * max_next_q - self.Q[state, action])\n",
        "\n",
        "# Example usage:\n",
        "ql = QLearning(num_states=100, num_actions=3)\n",
        "state = 0\n",
        "epsilon = 0.1\n",
        "action = ql.select_action(state, epsilon)\n",
        "reward = 1  # Assume positive reward for the selected action\n",
        "next_state = 1\n",
        "alpha = 0.1\n",
        "gamma = 0.9\n",
        "ql.update_q_value(state, action, reward, next_state, alpha, gamma)\n",
        "print(\"Updated Q-Table:\", ql.Q)"
      ],
      "metadata": {
        "id": "0rdrNAk01mLI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}