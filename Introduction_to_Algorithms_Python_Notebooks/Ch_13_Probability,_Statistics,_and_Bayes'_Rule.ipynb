{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Randomized QuickSort"
      ],
      "metadata": {
        "id": "dLB6xUMmH4k1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRkGk6pWGowC",
        "outputId": "c7b349d6-ed08-41e6-d256-101472758457"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 1, 2, 3, 6, 8, 10]\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "def RandomizedQuickSort(A, p, r):\n",
        "    \"\"\"\n",
        "    Randomized QuickSort algorithm to sort an array.\n",
        "\n",
        "    Args:\n",
        "    A: The array to be sorted.\n",
        "    p: The starting index of the subarray to be sorted.\n",
        "    r: The ending index of the subarray to be sorted.\n",
        "    \"\"\"\n",
        "    if p < r:\n",
        "        q = RandomPartition(A, p, r)\n",
        "        RandomizedQuickSort(A, p, q - 1)\n",
        "        RandomizedQuickSort(A, q + 1, r)\n",
        "\n",
        "def RandomPartition(A, p, r):\n",
        "    \"\"\"\n",
        "    Randomly partitions the array around a pivot element.\n",
        "\n",
        "    Args:\n",
        "    A: The array to be partitioned.\n",
        "    p: The starting index of the subarray to be partitioned.\n",
        "    r: The ending index of the subarray to be partitioned.\n",
        "\n",
        "    Returns:\n",
        "    The index of the pivot element after partitioning.\n",
        "    \"\"\"\n",
        "    i = random.randint(p, r)\n",
        "    A[i], A[r] = A[r], A[i]\n",
        "    return Partition(A, p, r)\n",
        "\n",
        "def Partition(A, p, r):\n",
        "    \"\"\"\n",
        "    Partitions the array around the pivot element.\n",
        "\n",
        "    Args:\n",
        "    A: The array to be partitioned.\n",
        "    p: The starting index of the subarray to be partitioned.\n",
        "    r: The ending index of the subarray to be partitioned.\n",
        "\n",
        "    Returns:\n",
        "    The index of the pivot element after partitioning.\n",
        "    \"\"\"\n",
        "    x = A[r]\n",
        "    i = p - 1\n",
        "    for j in range(p, r):\n",
        "        if A[j] <= x:\n",
        "            i += 1\n",
        "            A[i], A[j] = A[j], A[i]\n",
        "    A[i + 1], A[r] = A[r], A[i + 1]\n",
        "    return i + 1\n",
        "\n",
        "# Example usage:\n",
        "A = [3, 6, 8, 10, 1, 2, 1]\n",
        "RandomizedQuickSort(A, 0, len(A) - 1)\n",
        "print(A)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Monte Carlo Estimation"
      ],
      "metadata": {
        "id": "6aCtpctgINQA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def monte_carlo_estimation(N):\n",
        "    \"\"\"\n",
        "    Monte Carlo estimation to compute the expected value of a function.\n",
        "\n",
        "    Args:\n",
        "    N: The number of samples.\n",
        "\n",
        "    Returns:\n",
        "    The estimated mean (expected value) of the function f(x).\n",
        "    \"\"\"\n",
        "    S = 0\n",
        "    for i in range(1, N + 1):\n",
        "        X_i = random.random()  # Generate random sample\n",
        "        f_X_i = f(X_i)  # Compute f(X_i)\n",
        "        S += f_X_i  # Update sum S\n",
        "    mu_hat = S / N  # Compute the estimated mean\n",
        "    return mu_hat\n",
        "\n",
        "# Define the function f(x)\n",
        "def f(x):\n",
        "    \"\"\"\n",
        "    Function to be estimated.\n",
        "\n",
        "    Args:\n",
        "    x: Input value.\n",
        "\n",
        "    Returns:\n",
        "    The value of the function f(x).\n",
        "    \"\"\"\n",
        "    # Example function, replace with your own function definition\n",
        "    return x ** 2\n",
        "\n",
        "# Example usage\n",
        "N = 1000  # Number of samples\n",
        "estimated_mean = monte_carlo_estimation(N)\n",
        "print(\"Estimated mean:\", estimated_mean)"
      ],
      "metadata": {
        "id": "HAQ6N-UdINwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Las Vegas Algorithm"
      ],
      "metadata": {
        "id": "zDqrxb24IZpd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def LasVegasAlgorithm():\n",
        "    \"\"\"\n",
        "    Las Vegas algorithm that repeatedly executes a randomized algorithm until a termination condition is met.\n",
        "\n",
        "    Returns:\n",
        "    The result of the algorithm once the termination condition is met.\n",
        "    \"\"\"\n",
        "    result = None\n",
        "    while not termination_condition_met():\n",
        "        # Make random choices\n",
        "        # Execute algorithm\n",
        "        # Update result if necessary\n",
        "        result = execute_algorithm()\n",
        "    return result\n",
        "\n",
        "def termination_condition_met():\n",
        "    \"\"\"\n",
        "    Define your termination condition here.\n",
        "\n",
        "    Returns:\n",
        "    A boolean indicating whether the termination condition is met.\n",
        "    \"\"\"\n",
        "    # Example: Replace with actual termination condition\n",
        "    return False\n",
        "\n",
        "def execute_algorithm():\n",
        "    \"\"\"\n",
        "    Implement your algorithm here.\n",
        "\n",
        "    Returns:\n",
        "    The result of your algorithm.\n",
        "    \"\"\"\n",
        "    # Example: Replace with the actual algorithm\n",
        "    return random.choice([True, False])  # Placeholder example\n",
        "\n",
        "# Example usage:\n",
        "result = LasVegasAlgorithm()\n",
        "print(\"Result:\", result)"
      ],
      "metadata": {
        "id": "dhKRwGY6IaCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Expectation Maximization Algorithm"
      ],
      "metadata": {
        "id": "6vOorantIl26"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def expectation_maximization():\n",
        "    \"\"\"\n",
        "    Expectation-Maximization (EM) algorithm to estimate model parameters.\n",
        "\n",
        "    This function repeatedly performs the expectation and maximization steps\n",
        "    until convergence criteria are met.\n",
        "    \"\"\"\n",
        "    # Initialize model parameters\n",
        "    initialize_model_parameters()\n",
        "\n",
        "    converged = False\n",
        "    while not converged:\n",
        "        # Expectation Step: Compute expected values of hidden variables\n",
        "        compute_expected_values()\n",
        "\n",
        "        # Maximization Step: Update model parameters to maximize likelihood\n",
        "        update_model_parameters()\n",
        "\n",
        "        # Check for convergence (e.g., change in likelihood)\n",
        "        if convergence_criteria_met():\n",
        "            converged = True\n",
        "\n",
        "def initialize_model_parameters():\n",
        "    \"\"\"\n",
        "    Initialize model parameters.\n",
        "\n",
        "    This function performs the initialization of model parameters.\n",
        "    \"\"\"\n",
        "    # Example: Initialize parameters randomly or based on some heuristic\n",
        "    global parameters\n",
        "    parameters = {\n",
        "        'mean': random.random(),\n",
        "        'variance': random.random(),\n",
        "        'mixing_coeff': random.random()\n",
        "    }\n",
        "\n",
        "def compute_expected_values():\n",
        "    \"\"\"\n",
        "    Compute expected values of hidden variables.\n",
        "\n",
        "    This function performs the expectation step to compute the expected values of hidden variables.\n",
        "    \"\"\"\n",
        "    # Example: Compute expected values based on current parameters\n",
        "    global expected_values\n",
        "    expected_values = {\n",
        "        'latent_var_1': parameters['mean'] * 0.5,\n",
        "        'latent_var_2': parameters['variance'] * 0.5\n",
        "    }\n",
        "\n",
        "def update_model_parameters():\n",
        "    \"\"\"\n",
        "    Update model parameters.\n",
        "\n",
        "    This function performs the maximization step to update model parameters.\n",
        "    \"\"\"\n",
        "    # Example: Update parameters to maximize likelihood\n",
        "    global parameters\n",
        "    parameters['mean'] += 0.1\n",
        "    parameters['variance'] += 0.1\n",
        "    parameters['mixing_coeff'] = 1 - parameters['mixing_coeff']\n",
        "\n",
        "def convergence_criteria_met():\n",
        "    \"\"\"\n",
        "    Check if convergence criteria are met.\n",
        "\n",
        "    This function checks if the convergence criteria are met (e.g., change in likelihood).\n",
        "\n",
        "    Returns:\n",
        "    A boolean indicating whether the convergence criteria are met.\n",
        "    \"\"\"\n",
        "    # Example: Check if the parameters have converged\n",
        "    global parameters\n",
        "    return abs(parameters['mean'] - parameters['variance']) < 0.01\n",
        "\n",
        "# Example usage\n",
        "expectation_maximization()"
      ],
      "metadata": {
        "id": "s_BxnkLrImYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sample Mean Estimation"
      ],
      "metadata": {
        "id": "PXl_KZNrIz6H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_mean_estimation(sample_data):\n",
        "    \"\"\"\n",
        "    Estimate the sample mean from the given sample data.\n",
        "\n",
        "    Args:\n",
        "    sample_data: A list of numerical values representing the sample data.\n",
        "\n",
        "    Returns:\n",
        "    The sample mean of the given data.\n",
        "    \"\"\"\n",
        "    n = len(sample_data)  # Number of samples\n",
        "    S = sum(sample_data)  # Initialize sum S\n",
        "    sample_mean = S / n  # Compute sample mean \\bar{X}\n",
        "    return sample_mean\n",
        "\n",
        "# Example usage:\n",
        "sample_data = [10, 20, 30, 40, 50]\n",
        "sample_mean = sample_mean_estimation(sample_data)\n",
        "print(\"Sample Mean:\", sample_mean)"
      ],
      "metadata": {
        "id": "RJJh3n_PI0V0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Confidence Interval Estimation"
      ],
      "metadata": {
        "id": "74gMhRoTI_nk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import norm\n",
        "\n",
        "def confidence_interval_estimation(data, alpha=0.05):\n",
        "    \"\"\"\n",
        "    Estimate the confidence interval for the mean of the given data.\n",
        "\n",
        "    Args:\n",
        "    data: A list or array of numerical values representing the sample data.\n",
        "    alpha: The significance level (default is 0.05 for a 95% confidence interval).\n",
        "\n",
        "    Returns:\n",
        "    The lower and upper bounds of the confidence interval.\n",
        "    \"\"\"\n",
        "    n = len(data)  # Number of samples\n",
        "    sample_mean = np.mean(data)  # Sample mean\n",
        "    sample_std = np.std(data, ddof=1)  # Sample standard deviation (ddof=1 for sample std deviation)\n",
        "    z_alpha_half = norm.ppf(1 - alpha / 2)  # z-value for the given confidence level\n",
        "\n",
        "    margin_of_error = z_alpha_half * sample_std / np.sqrt(n)  # Margin of error\n",
        "    lower_bound = sample_mean - margin_of_error  # Lower bound of confidence interval\n",
        "    upper_bound = sample_mean + margin_of_error  # Upper bound of confidence interval\n",
        "\n",
        "    return lower_bound, upper_bound\n",
        "\n",
        "# Example usage:\n",
        "sample_data = [10, 15, 20, 25, 30]\n",
        "confidence_level = 0.95\n",
        "lower_bound, upper_bound = confidence_interval_estimation(sample_data, alpha=1 - confidence_level)\n",
        "print(\"Confidence Interval:\", (lower_bound, upper_bound))"
      ],
      "metadata": {
        "id": "N1npdMvwI__a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Walk on a Graph"
      ],
      "metadata": {
        "id": "HPZDKtQQJWIO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def random_walk(graph, starting_vertex, T):\n",
        "    \"\"\"\n",
        "    Perform a random walk on a graph.\n",
        "\n",
        "    Args:\n",
        "    graph: A dictionary representing the graph where keys are vertex names and values are lists of adjacent vertices.\n",
        "    starting_vertex: The vertex where the random walk starts.\n",
        "    T: The number of steps in the random walk.\n",
        "\n",
        "    Returns:\n",
        "    The vertex reached after T steps or the last vertex if the walk terminates early due to no neighbors.\n",
        "    \"\"\"\n",
        "    current_vertex = starting_vertex\n",
        "    for t in range(T):\n",
        "        neighbors = graph.get(current_vertex, [])  # Get the neighbors of the current vertex\n",
        "        if neighbors:\n",
        "            next_vertex = random.choice(neighbors)  # Choose a random neighbor\n",
        "            current_vertex = next_vertex  # Move to the next vertex\n",
        "        else:\n",
        "            break  # If there are no neighbors, terminate the walk\n",
        "    return current_vertex\n",
        "\n",
        "# Example usage:\n",
        "graph = {\n",
        "    'A': ['B', 'C'],\n",
        "    'B': ['A', 'C', 'D'],\n",
        "    'C': ['A', 'B', 'D'],\n",
        "    'D': ['B', 'C']\n",
        "}\n",
        "starting_vertex = 'A'\n",
        "T = 10\n",
        "final_vertex = random_walk(graph, starting_vertex, T)\n",
        "print(\"Final Vertex after Random Walk:\", final_vertex)"
      ],
      "metadata": {
        "id": "X0jeO5A_JW1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bayesian Linear Regression"
      ],
      "metadata": {
        "id": "ETF_6D9_J1jx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "\n",
        "def bayesian_linear_regression(X, Y, prior_mean, prior_variance, noise_variance, num_samples=1000):\n",
        "    \"\"\"\n",
        "    Perform Bayesian linear regression to estimate the parameters and make predictions.\n",
        "\n",
        "    Args:\n",
        "    X: The feature matrix.\n",
        "    Y: The target values.\n",
        "    prior_mean: The prior mean for the parameters theta.\n",
        "    prior_variance: The prior variance for the parameters theta.\n",
        "    noise_variance: The variance of the noise in the data.\n",
        "    num_samples: The number of samples to draw from the posterior distribution (default is 1000).\n",
        "\n",
        "    Returns:\n",
        "    estimated_parameters: The estimated parameters theta with the highest posterior probability.\n",
        "    predicted_Y_new: The predicted target values for the given feature matrix X.\n",
        "    \"\"\"\n",
        "    # Prior distribution for parameters theta\n",
        "    prior_distribution = stats.norm(loc=prior_mean, scale=np.sqrt(prior_variance))\n",
        "\n",
        "    # Likelihood function P(Y | X, theta)\n",
        "    def likelihood(theta):\n",
        "        predicted_Y = np.dot(X, theta)\n",
        "        likelihoods = stats.norm.logpdf(Y, loc=predicted_Y, scale=np.sqrt(noise_variance))\n",
        "        return np.sum(likelihoods)\n",
        "\n",
        "    # Calculate posterior distribution P(theta | X, Y) using Bayes' theorem\n",
        "    def posterior(theta):\n",
        "        return prior_distribution.logpdf(theta).sum() + likelihood(theta)\n",
        "\n",
        "    # Sample from the prior distribution\n",
        "    samples = np.random.normal(prior_mean, np.sqrt(prior_variance), size=(num_samples, len(prior_mean)))\n",
        "    posterior_samples = [(sample, posterior(sample)) for sample in samples]\n",
        "\n",
        "    # Sort samples by their posterior probability in descending order\n",
        "    posterior_samples.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Use estimated parameters to make predictions for new data points\n",
        "    estimated_parameters = posterior_samples[0][0]  # Take the sample with highest posterior probability\n",
        "    predicted_Y_new = np.dot(X, estimated_parameters)\n",
        "\n",
        "    return estimated_parameters, predicted_Y_new\n",
        "\n",
        "# Example usage:\n",
        "X = np.array([[1, 2], [3, 4], [5, 6]])  # Example feature matrix\n",
        "Y = np.array([3, 4, 5])  # Example target values\n",
        "prior_mean = np.array([0, 0])  # Example prior mean for parameters theta\n",
        "prior_variance = 1.0  # Example prior variance for parameters theta\n",
        "noise_variance = 0.1  # Example noise variance\n",
        "estimated_parameters, predicted_Y_new = bayesian_linear_regression(X, Y, prior_mean, prior_variance, noise_variance)\n",
        "print(\"Estimated Parameters:\", estimated_parameters)\n",
        "print(\"Predicted Y for New Data Points:\", predicted_Y_new)"
      ],
      "metadata": {
        "id": "eVGL8tQvJ2B8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Belief Propagation in Bayesian Networks"
      ],
      "metadata": {
        "id": "RVIVgsROKGzM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def belief_propagation(bayesian_network, max_iterations=100, tolerance=1e-6):\n",
        "    \"\"\"\n",
        "    Perform belief propagation in a Bayesian network.\n",
        "\n",
        "    Args:\n",
        "    bayesian_network: A list of nodes representing the Bayesian network. Each node has a list of neighbors.\n",
        "    max_iterations: The maximum number of iterations to run the algorithm (default is 100).\n",
        "    tolerance: The convergence tolerance (default is 1e-6).\n",
        "\n",
        "    Returns:\n",
        "    beliefs: The final beliefs for each node.\n",
        "    messages: The messages passed between nodes during belief propagation.\n",
        "    \"\"\"\n",
        "    # Initialize messages at each node\n",
        "    messages = initialize_messages(bayesian_network)\n",
        "    beliefs = np.zeros_like(messages)\n",
        "\n",
        "    # Convergence flag\n",
        "    converged = False\n",
        "    iteration = 0\n",
        "\n",
        "    while not converged and iteration < max_iterations:\n",
        "        converged = True\n",
        "\n",
        "        # Iterate over nodes in topological order\n",
        "        for i, node in enumerate(bayesian_network):\n",
        "            incoming_messages = []\n",
        "\n",
        "            # Collect incoming messages from neighbors\n",
        "            for neighbor in node.neighbors:\n",
        "                incoming_messages.append(messages[neighbor][i])\n",
        "\n",
        "            # Send message to node i from each neighbor\n",
        "            for neighbor in node.neighbors:\n",
        "                message_to_i = compute_message(beliefs, incoming_messages, node, neighbor)\n",
        "                if np.linalg.norm(messages[neighbor][i] - message_to_i) > tolerance:\n",
        "                    converged = False\n",
        "                messages[neighbor][i] = message_to_i\n",
        "\n",
        "            # Update beliefs at node i based on incoming messages\n",
        "            beliefs[i] = update_beliefs(node, incoming_messages)\n",
        "\n",
        "        iteration += 1\n",
        "\n",
        "    return beliefs, messages\n",
        "\n",
        "def initialize_messages(bayesian_network):\n",
        "    \"\"\"\n",
        "    Initialize messages for the Bayesian network.\n",
        "\n",
        "    Args:\n",
        "    bayesian_network: A list of nodes representing the Bayesian network.\n",
        "\n",
        "    Returns:\n",
        "    A numpy array of initial messages.\n",
        "    \"\"\"\n",
        "    num_nodes = len(bayesian_network)\n",
        "    return np.zeros((num_nodes, num_nodes))\n",
        "\n",
        "def compute_message(beliefs, incoming_messages, node_i, node_j):\n",
        "    \"\"\"\n",
        "    Compute the message from node_j to node_i.\n",
        "\n",
        "    Args:\n",
        "    beliefs: The current beliefs for each node.\n",
        "    incoming_messages: The incoming messages to node_i.\n",
        "    node_i: The target node.\n",
        "    node_j: The source node.\n",
        "\n",
        "    Returns:\n",
        "    The message from node_j to node_i.\n",
        "    \"\"\"\n",
        "    # Example computation of message from node_j to node_i\n",
        "    return np.ones_like(beliefs[node_i]) * 0.5\n",
        "\n",
        "def update_beliefs(node, incoming_messages):\n",
        "    \"\"\"\n",
        "    Update the beliefs at a node based on incoming messages.\n",
        "\n",
        "    Args:\n",
        "    node: The current node.\n",
        "    incoming_messages: The incoming messages to the node.\n",
        "\n",
        "    Returns:\n",
        "    The updated beliefs for the node.\n",
        "    \"\"\"\n",
        "    # Example update of beliefs at node_i based on incoming messages\n",
        "    return np.ones_like(incoming_messages[0])\n",
        "\n",
        "# Example usage:\n",
        "class Node:\n",
        "    def __init__(self, neighbors):\n",
        "        self.neighbors = neighbors\n",
        "\n",
        "# Example Bayesian Network represented as a list of nodes\n",
        "bayesian_network = [Node([1]), Node([0, 2]), Node([1])]\n",
        "\n",
        "beliefs, messages = belief_propagation(bayesian_network)\n",
        "print(\"Beliefs:\", beliefs)\n",
        "print(\"Messages:\", messages)"
      ],
      "metadata": {
        "id": "XgAA7By_KHOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PageRank Algorithm"
      ],
      "metadata": {
        "id": "lbL0NjfVK4WL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def pagerank(graph, damping_factor=0.85, max_iterations=100, tolerance=1e-6):\n",
        "    \"\"\"\n",
        "    Compute the PageRank of each node in the graph.\n",
        "\n",
        "    Args:\n",
        "    graph: A dictionary representing the graph where keys are nodes and values are lists of adjacent nodes.\n",
        "    damping_factor: The damping factor (default is 0.85).\n",
        "    max_iterations: The maximum number of iterations to run the algorithm (default is 100).\n",
        "    tolerance: The convergence tolerance (default is 1e-6).\n",
        "\n",
        "    Returns:\n",
        "    pagerank_values: A dictionary with nodes as keys and their corresponding PageRank values as values.\n",
        "    \"\"\"\n",
        "    num_nodes = len(graph)\n",
        "    pagerank_values = {node: 1 / num_nodes for node in graph}  # Initialize PageRank values\n",
        "    new_pagerank_values = pagerank_values.copy()\n",
        "\n",
        "    for iteration in range(max_iterations):\n",
        "        for node in graph:\n",
        "            rank_sum = 0\n",
        "            for neighbor in graph:\n",
        "                if node in graph[neighbor]:\n",
        "                    rank_sum += pagerank_values[neighbor] / len(graph[neighbor])\n",
        "            new_pagerank_values[node] = (1 - damping_factor) / num_nodes + damping_factor * rank_sum\n",
        "\n",
        "        # Check for convergence\n",
        "        if all(abs(new_pagerank_values[node] - pagerank_values[node]) < tolerance for node in graph):\n",
        "            break\n",
        "\n",
        "        pagerank_values = new_pagerank_values.copy()\n",
        "\n",
        "    return pagerank_values\n",
        "\n",
        "# Example usage:\n",
        "graph = {\n",
        "    'A': ['B', 'C'],\n",
        "    'B': ['C', 'D'],\n",
        "    'C': ['A'],\n",
        "    'D': ['C']\n",
        "}\n",
        "\n",
        "pagerank_values = pagerank(graph)\n",
        "print(\"PageRank Values:\", pagerank_values)"
      ],
      "metadata": {
        "id": "YHUtC5fPK5t5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Count-Min Sketch Algorithm"
      ],
      "metadata": {
        "id": "DEslOM8GK-1y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import hashlib\n",
        "\n",
        "class CountMinSketch:\n",
        "    def __init__(self, w, k):\n",
        "        \"\"\"\n",
        "        Initialize the Count-Min Sketch.\n",
        "\n",
        "        Args:\n",
        "        w: The width of the counters array.\n",
        "        k: The number of hash functions.\n",
        "        \"\"\"\n",
        "        self.w = w  # Width of the array\n",
        "        self.k = k  # Number of hash functions\n",
        "        self.C = [[0] * w for _ in range(k)]  # Initialize the counters array\n",
        "\n",
        "    def hash_functions(self, x):\n",
        "        \"\"\"\n",
        "        Generate k different hash values for the input x.\n",
        "\n",
        "        Args:\n",
        "        x: The input element to be hashed.\n",
        "\n",
        "        Returns:\n",
        "        A list of k hash values.\n",
        "        \"\"\"\n",
        "        hashes = []\n",
        "        for i in range(self.k):\n",
        "            # Use different hash functions by appending the index to the string\n",
        "            hashes.append(int(hashlib.md5((str(x) + str(i)).encode()).hexdigest(), 16) % self.w)\n",
        "        return hashes\n",
        "\n",
        "    def update(self, x):\n",
        "        \"\"\"\n",
        "        Update the Count-Min Sketch with the input x.\n",
        "\n",
        "        Args:\n",
        "        x: The input element to be added to the sketch.\n",
        "        \"\"\"\n",
        "        for i, hash_val in enumerate(self.hash_functions(x)):\n",
        "            self.C[i][hash_val] += 1\n",
        "\n",
        "    def query(self, x):\n",
        "        \"\"\"\n",
        "        Query the frequency of the input x in the Count-Min Sketch.\n",
        "\n",
        "        Args:\n",
        "        x: The input element to query.\n",
        "\n",
        "        Returns:\n",
        "        The estimated frequency of x.\n",
        "        \"\"\"\n",
        "        min_count = float('inf')\n",
        "        for i, hash_val in enumerate(self.hash_functions(x)):\n",
        "            min_count = min(min_count, self.C[i][hash_val])\n",
        "        return min_count\n",
        "\n",
        "# Example usage\n",
        "data_stream = [1, 2, 3, 4, 1, 2, 1, 2, 3, 4, 4, 4]\n",
        "w = 10  # Width of the array\n",
        "k = 5   # Number of hash functions\n",
        "cms = CountMinSketch(w, k)\n",
        "\n",
        "# Update the Count-Min Sketch with the data stream\n",
        "for element in data_stream:\n",
        "    cms.update(element)\n",
        "\n",
        "# Query the frequency of elements\n",
        "query_elements = [1, 2, 3, 4, 5]\n",
        "for element in query_elements:\n",
        "    print(\"Frequency of\", element, \":\", cms.query(element))\n"
      ],
      "metadata": {
        "id": "u9yPJ5V9K_M-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Naive Bayes Classifier"
      ],
      "metadata": {
        "id": "93TuSbRRYk9q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load the dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Gaussian Naive Bayes classifier\n",
        "gnb = GaussianNB()\n",
        "\n",
        "# Train the classifier\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Classification Report:\")\n",
        "print(report)"
      ],
      "metadata": {
        "id": "F1TMj_CBYlXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Metropolis-Hastings Algorithm"
      ],
      "metadata": {
        "id": "CednYZvDYnij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def target_distribution(x):\n",
        "    # Example target distribution: Standard normal distribution\n",
        "    return np.exp(-0.5 * x**2) / np.sqrt(2 * np.pi)\n",
        "\n",
        "def proposal_distribution(x, sigma):\n",
        "    # Example proposal distribution: Normal distribution centered at x with standard deviation sigma\n",
        "    return np.random.normal(x, sigma)\n",
        "\n",
        "def metropolis_hastings(target, proposal, initial_value, num_samples, sigma):\n",
        "    samples = [initial_value]\n",
        "    current_value = initial_value\n",
        "\n",
        "    for _ in range(num_samples):\n",
        "        proposed_value = proposal(current_value, sigma)\n",
        "        acceptance_ratio = target(proposed_value) / target(current_value)\n",
        "        if np.random.rand() < acceptance_ratio:\n",
        "            current_value = proposed_value\n",
        "        samples.append(current_value)\n",
        "\n",
        "    return np.array(samples)\n",
        "\n",
        "# Parameters\n",
        "initial_value = 0\n",
        "num_samples = 10000\n",
        "sigma = 1.0\n",
        "\n",
        "# Run Metropolis-Hastings algorithm\n",
        "samples = metropolis_hastings(target_distribution, proposal_distribution, initial_value, num_samples, sigma)\n",
        "\n",
        "# Plot results\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.hist(samples, bins=50, density=True, alpha=0.6, color='g', label='Samples')\n",
        "x = np.linspace(-4, 4, 1000)\n",
        "plt.plot(x, target_distribution(x), 'r', lw=2, label='Target Distribution')\n",
        "plt.title('Metropolis-Hastings Sampling')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('Density')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ku7mR8XHYpBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rejection ABC Algorithm"
      ],
      "metadata": {
        "id": "rhu0jWxUYqqX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# True parameters of the distribution (unknown to the algorithm)\n",
        "true_mu = 2.0\n",
        "true_sigma = 1.0\n",
        "\n",
        "# Simulated observed data (known to the algorithm)\n",
        "observed_data = np.random.normal(true_mu, true_sigma, size=100)\n",
        "\n",
        "# Function to simulate data from a given parameter set\n",
        "def simulate_data(mu, sigma, size=100):\n",
        "    return np.random.normal(mu, sigma, size)\n",
        "\n",
        "# Function to calculate the summary statistics (Euclidean distance in this case)\n",
        "def summary_statistics(data):\n",
        "    return np.mean(data), np.std(data)\n",
        "\n",
        "# Function to compute distance between observed and simulated summary statistics\n",
        "def compute_distance(obs_stat, sim_stat):\n",
        "    obs_mean, obs_std = obs_stat\n",
        "    sim_mean, sim_std = sim_stat\n",
        "    return np.sqrt((obs_mean - sim_mean)**2 + (obs_std - sim_std)**2)\n",
        "\n",
        "# Rejection ABC algorithm\n",
        "def rejection_abc(observed_data, epsilon, num_samples):\n",
        "    samples = []\n",
        "    while len(samples) < num_samples:\n",
        "        # Generate parameters from prior distributions (uniform here for simplicity)\n",
        "        mu = np.random.uniform(0, 5)\n",
        "        sigma = np.random.uniform(0, 3)\n",
        "\n",
        "        # Simulate data from the current parameter set\n",
        "        simulated_data = simulate_data(mu, sigma)\n",
        "\n",
        "        # Calculate summary statistics of the simulated data\n",
        "        sim_stats = summary_statistics(simulated_data)\n",
        "\n",
        "        # Calculate distance between observed and simulated summary statistics\n",
        "        distance = compute_distance(summary_statistics(observed_data), sim_stats)\n",
        "\n",
        "        # Accept the parameter set if the distance is less than epsilon\n",
        "        if distance < epsilon:\n",
        "            samples.append((mu, sigma))\n",
        "\n",
        "    return np.array(samples)\n",
        "\n",
        "# Parameters\n",
        "epsilon = 0.5  # Acceptance threshold\n",
        "num_samples = 1000  # Number of samples to generate\n",
        "\n",
        "# Run Rejection ABC algorithm\n",
        "samples = rejection_abc(observed_data, epsilon, num_samples)\n",
        "\n",
        "# Plot results\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(samples[:, 0], samples[:, 1], alpha=0.5, label='Samples')\n",
        "plt.axvline(true_mu, color='r', linestyle='--', label='True $\\mu$')\n",
        "plt.axhline(true_sigma, color='b', linestyle='--', label='True $\\sigma$')\n",
        "plt.xlabel('$\\mu$')\n",
        "plt.ylabel('$\\sigma$')\n",
        "plt.title('Rejection ABC Sampling')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ywakCtMVYsfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Probabilistic Inference in Bayesian Networks"
      ],
      "metadata": {
        "id": "vICYIMrTYuQW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pgmpy.models import BayesianNetwork\n",
        "from pgmpy.factors.discrete import TabularCPD\n",
        "from pgmpy.inference import VariableElimination\n",
        "\n",
        "# Define the Bayesian Network structure\n",
        "bayesian_network = BayesianNetwork([('A', 'C'), ('B', 'C'), ('C', 'D')])\n",
        "\n",
        "# Define Conditional Probability Distributions (CPDs)\n",
        "cpd_a = TabularCPD(variable='A', variable_card=2, values=[[0.7], [0.3]])\n",
        "cpd_b = TabularCPD(variable='B', variable_card=2, values=[[0.8], [0.2]])\n",
        "cpd_c = TabularCPD(variable='C', variable_card=2,\n",
        "                   values=[[0.9, 0.8, 0.5, 0.4],\n",
        "                           [0.1, 0.2, 0.5, 0.6]],\n",
        "                   evidence=['A', 'B'], evidence_card=[2, 2])\n",
        "cpd_d = TabularCPD(variable='D', variable_card=2,\n",
        "                   values=[[0.3, 0.2],\n",
        "                           [0.7, 0.8]],\n",
        "                   evidence=['C'], evidence_card=[2])\n",
        "\n",
        "# Add CPDs to the Bayesian Network\n",
        "bayesian_network.add_cpds(cpd_a, cpd_b, cpd_c, cpd_d)\n",
        "\n",
        "# Check if the CPDs are valid\n",
        "print(\"CPDs valid:\", bayesian_network.check_model())\n",
        "\n",
        "# Perform variable elimination for inference\n",
        "inference = VariableElimination(bayesian_network)\n",
        "\n",
        "# Calculate the probability of variables given evidence\n",
        "query = inference.query(variables=['D'], evidence={'A': 1, 'B': 0})\n",
        "print(\"P(D | A=1, B=0):\")\n",
        "print(query)\n",
        "\n",
        "# You can query for different variables and evidence combinations as needed"
      ],
      "metadata": {
        "id": "o4TbrIwlYv4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Metropolis-Hastings Algorithm for Bayesian Linear Regression"
      ],
      "metadata": {
        "id": "7aRpaMhwYxuU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate synthetic data\n",
        "np.random.seed(0)\n",
        "num_samples = 100\n",
        "true_slope = 2.0\n",
        "true_intercept = 1.0\n",
        "noise_std = 1.0\n",
        "x = np.random.uniform(-5, 5, num_samples)\n",
        "y = true_slope * x + true_intercept + np.random.normal(0, noise_std, num_samples)\n",
        "\n",
        "# Bayesian Linear Regression using Metropolis-Hastings Algorithm\n",
        "def metropolis_hastings_bayesian_linear_regression(x, y, num_samples=10000, burn_in=1000):\n",
        "    # Define prior parameters (Normal distribution)\n",
        "    prior_mean = np.zeros(2)  # Mean of the prior\n",
        "    prior_cov = np.diag([1.0, 1.0])  # Covariance matrix of the prior\n",
        "\n",
        "    # Initialize parameters\n",
        "    current_beta = np.zeros(2)  # Initial guess for beta (slope and intercept)\n",
        "    beta_samples = []\n",
        "\n",
        "    # Function to calculate log likelihood\n",
        "    def log_likelihood(beta):\n",
        "        predicted = beta[0] * x + beta[1]\n",
        "        return -0.5 * np.sum((y - predicted) ** 2) / noise_std**2\n",
        "\n",
        "    # Function to calculate log prior\n",
        "    def log_prior(beta):\n",
        "        return -0.5 * (beta - prior_mean).T @ np.linalg.inv(prior_cov) @ (beta - prior_mean)\n",
        "\n",
        "    # Initial log posterior\n",
        "    current_log_posterior = log_likelihood(current_beta) + log_prior(current_beta)\n",
        "\n",
        "    # Metropolis-Hastings sampling\n",
        "    for _ in range(num_samples + burn_in):\n",
        "        # Generate proposal from a normal distribution centered at current_beta\n",
        "        proposal = current_beta + np.random.normal(0, 0.1, size=2)\n",
        "\n",
        "        # Calculate log posterior of the proposed beta\n",
        "        proposal_log_posterior = log_likelihood(proposal) + log_prior(proposal)\n",
        "\n",
        "        # Accept or reject the proposal\n",
        "        acceptance_prob = min(1, np.exp(proposal_log_posterior - current_log_posterior))\n",
        "        if np.random.uniform() < acceptance_prob:\n",
        "            current_beta = proposal\n",
        "            current_log_posterior = proposal_log_posterior\n",
        "\n",
        "        # Save samples after burn-in\n",
        "        if _ >= burn_in:\n",
        "            beta_samples.append(current_beta)\n",
        "\n",
        "    return np.array(beta_samples)\n",
        "\n",
        "# Run Metropolis-Hastings algorithm for Bayesian Linear Regression\n",
        "samples = metropolis_hastings_bayesian_linear_regression(x, y)\n",
        "\n",
        "# Plotting results\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(samples[:, 0], label='slope (beta1)')\n",
        "plt.axhline(true_slope, color='r', linestyle='--', label='True slope')\n",
        "plt.title('Samples of Slope (beta1)')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(samples[:, 1], label='intercept (beta0)')\n",
        "plt.axhline(true_intercept, color='r', linestyle='--', label='True intercept')\n",
        "plt.title('Samples of Intercept (beta0)')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YJwavE3VYzbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bayesian Spam Filtering Algorithm"
      ],
      "metadata": {
        "id": "J9Oy35sOY2Xu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "\n",
        "class BayesianSpamFilter:\n",
        "    def __init__(self):\n",
        "        self.spam_word_counts = defaultdict(int)\n",
        "        self.ham_word_counts = defaultdict(int)\n",
        "        self.spam_total_count = 0\n",
        "        self.ham_total_count = 0\n",
        "        self.spam_prior = 0.5  # Prior probability of an email being spam\n",
        "        self.word_threshold = 5  # Threshold count to consider a word as a feature\n",
        "\n",
        "    def train(self, emails, labels):\n",
        "        \"\"\"\n",
        "        Train the Bayesian Spam Filter with a set of emails and their corresponding labels.\n",
        "\n",
        "        :param emails: List of email texts\n",
        "        :param labels: List of labels (1 for spam, 0 for ham)\n",
        "        \"\"\"\n",
        "        for email, label in zip(emails, labels):\n",
        "            if label == 1:\n",
        "                self.spam_total_count += 1\n",
        "                for word in self.extract_words(email):\n",
        "                    self.spam_word_counts[word] += 1\n",
        "            else:\n",
        "                self.ham_total_count += 1\n",
        "                for word in self.extract_words(email):\n",
        "                    self.ham_word_counts[word] += 1\n",
        "\n",
        "    def extract_words(self, email):\n",
        "        \"\"\"\n",
        "        Extract words from an email after cleaning and normalization.\n",
        "\n",
        "        :param email: Email text\n",
        "        :return: List of words (features)\n",
        "        \"\"\"\n",
        "        words = re.findall(r'\\b\\w+\\b', email.lower())\n",
        "        return words\n",
        "\n",
        "    def calculate_word_probabilities(self):\n",
        "        \"\"\"\n",
        "        Calculate probabilities of each word being spam or ham using Bayesian estimation.\n",
        "\n",
        "        :return: Dictionary of word probabilities\n",
        "        \"\"\"\n",
        "        word_probabilities = {}\n",
        "        for word in set(self.spam_word_counts.keys()).union(set(self.ham_word_counts.keys())):\n",
        "            spam_count = self.spam_word_counts[word]\n",
        "            ham_count = self.ham_word_counts[word]\n",
        "\n",
        "            if spam_count + ham_count >= self.word_threshold:\n",
        "                spam_probability = (spam_count / float(self.spam_total_count)) if self.spam_total_count > 0 else 0\n",
        "                ham_probability = (ham_count / float(self.ham_total_count)) if self.ham_total_count > 0 else 0\n",
        "\n",
        "                # Using Laplace (add-one) smoothing for better estimation\n",
        "                smoothing_factor = 1  # Laplace smoothing factor\n",
        "                spam_probability_smoothed = (spam_count + smoothing_factor) / \\\n",
        "                                            float(self.spam_total_count + 2 * smoothing_factor)\n",
        "                ham_probability_smoothed = (ham_count + smoothing_factor) / \\\n",
        "                                           float(self.ham_total_count + 2 * smoothing_factor)\n",
        "\n",
        "                word_probabilities[word] = {\n",
        "                    'spam_probability': spam_probability_smoothed,\n",
        "                    'ham_probability': ham_probability_smoothed\n",
        "                }\n",
        "\n",
        "        return word_probabilities\n",
        "\n",
        "    def predict(self, email):\n",
        "        \"\"\"\n",
        "        Predict whether an email is spam or ham based on its features (words).\n",
        "\n",
        "        :param email: Email text\n",
        "        :return: Predicted label (1 for spam, 0 for ham)\n",
        "        \"\"\"\n",
        "        words = self.extract_words(email)\n",
        "        word_probabilities = self.calculate_word_probabilities()\n",
        "\n",
        "        log_spam_probability = np.log(self.spam_prior)\n",
        "        log_ham_probability = np.log(1 - self.spam_prior)\n",
        "\n",
        "        for word in words:\n",
        "            if word in word_probabilities:\n",
        "                log_spam_probability += np.log(word_probabilities[word]['spam_probability'])\n",
        "                log_ham_probability += np.log(word_probabilities[word]['ham_probability'])\n",
        "\n",
        "        if log_spam_probability > log_ham_probability:\n",
        "            return 1  # Spam\n",
        "        else:\n",
        "            return 0  # Ham\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    # Example data (emails and labels)\n",
        "    emails = [\n",
        "        \"Buy cheap Viagra now!!!\",\n",
        "        \"Hello, how are you?\",\n",
        "        \"Get free money!\",\n",
        "        \"Dear friend, please send the documents.\"\n",
        "    ]\n",
        "    labels = [1, 0, 1, 0]  # 1 for spam, 0 for ham\n",
        "\n",
        "    # Initialize and train the Bayesian Spam Filter\n",
        "    filter = BayesianSpamFilter()\n",
        "    filter.train(emails, labels)\n",
        "\n",
        "    # Test the filter with new emails\n",
        "    test_emails = [\n",
        "        \"Buy now, limited offer!\",\n",
        "        \"Hi, just checking in.\",\n",
        "        \"Send me your bank details.\",\n",
        "        \"Congratulations! You've won a prize.\"\n",
        "    ]\n",
        "\n",
        "    for email in test_emails:\n",
        "        prediction = filter.predict(email)\n",
        "        if prediction == 1:\n",
        "            print(f\"'{email}' is predicted as spam.\")\n",
        "        else:\n",
        "            print(f\"'{email}' is predicted as ham.\")"
      ],
      "metadata": {
        "id": "u3zEhw87Y2vb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hierarchical Bayesian Model"
      ],
      "metadata": {
        "id": "TOTzEU68Y5y0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pymc3 as pm\n",
        "\n",
        "# Generate simulated data\n",
        "np.random.seed(42)\n",
        "num_groups = 3\n",
        "group_sizes = np.random.randint(10, 20, num_groups)\n",
        "data = []\n",
        "for i, size in enumerate(group_sizes):\n",
        "    x = np.random.randn(size)\n",
        "    y = 2 * x + np.random.randn(size)\n",
        "    data.extend(zip(x, y, [i] * size))\n",
        "\n",
        "x, y, groups = zip(*data)\n",
        "x, y, groups = np.array(x), np.array(y), np.array(groups)\n",
        "\n",
        "# Hierarchical linear regression model\n",
        "with pm.Model() as hierarchical_model:\n",
        "    # Hyperpriors\n",
        "    mu_alpha = pm.Normal('mu_alpha', mu=0, sd=10)\n",
        "    sigma_alpha = pm.HalfNormal('sigma_alpha', sd=10)\n",
        "    mu_beta = pm.Normal('mu_beta', mu=0, sd=10)\n",
        "    sigma_beta = pm.HalfNormal('sigma_beta', sd=10)\n",
        "\n",
        "    # Group-level parameters\n",
        "    alpha = pm.Normal('alpha', mu=mu_alpha, sd=sigma_alpha, shape=num_groups)\n",
        "    beta = pm.Normal('beta', mu=mu_beta, sd=sigma_beta, shape=num_groups)\n",
        "\n",
        "    # Likelihood\n",
        "    mu = alpha[groups] + beta[groups] * x\n",
        "    sigma = pm.HalfNormal('sigma', sd=1)\n",
        "    y_obs = pm.Normal('y_obs', mu=mu, sd=sigma, observed=y)\n",
        "\n",
        "# Sampling\n",
        "with hierarchical_model:\n",
        "    trace = pm.sample(2000, tune=2000)\n",
        "\n",
        "# Posterior analysis\n",
        "pm.summary(trace)\n",
        "pm.traceplot(trace)"
      ],
      "metadata": {
        "id": "CyvP9ZkCY6Ap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation of Bayesian Non-parametrics"
      ],
      "metadata": {
        "id": "BVuId6XuY878"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import norm\n",
        "from sklearn.mixture import BayesianGaussianMixture\n",
        "\n",
        "# Generate simulated data\n",
        "np.random.seed(42)\n",
        "data = np.concatenate([\n",
        "    np.random.normal(-5, 1, 300),\n",
        "    np.random.normal(0, 1, 300),\n",
        "    np.random.normal(5, 1, 400)\n",
        "])\n",
        "\n",
        "# Fit Dirichlet Process Mixture Model\n",
        "dpgmm = BayesianGaussianMixture(n_components=10, covariance_type='full')\n",
        "dpgmm.fit(data.reshape(-1, 1))\n",
        "\n",
        "# Plot results\n",
        "x = np.linspace(-10, 10, 1000)\n",
        "plt.hist(data, bins=30, density=True, alpha=0.5, label='Histogram of data')\n",
        "for i in range(dpgmm.n_components):\n",
        "    y = np.exp(dpgmm.weights_[i]) * norm.pdf(x, dpgmm.means_[i, 0], np.sqrt(dpgmm.covariances_[i, 0, 0]))\n",
        "    plt.plot(x, y, label=f'Component {i}')\n",
        "plt.title('Bayesian Gaussian Mixture Model')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8hE9Y-ziY9JQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation of Bayesian Decision Theory"
      ],
      "metadata": {
        "id": "BPD9rEaKZARM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define States of the world and actions\n",
        "States = ['sunny', 'cloudy', 'rainy']\n",
        "actions = ['go_outside', 'stay_inside']\n",
        "\n",
        "# Prior probabilities over States\n",
        "prior_probs = np.array([0.4, 0.3, 0.3])\n",
        "\n",
        "# Conditional probabilities of actions given States\n",
        "action_probs = np.array([\n",
        "    [0.9, 0.1],\n",
        "    [0.5, 0.5],\n",
        "    [0.2, 0.8]\n",
        "])\n",
        "\n",
        "# Utility function\n",
        "utility = np.array([\n",
        "    [1, 0],\n",
        "    [0, 1],\n",
        "    [0, -1]\n",
        "])\n",
        "\n",
        "# Calculate expected utilities\n",
        "expected_utilities = np.dot(prior_probs, action_probs * utility)\n",
        "\n",
        "# Make decision\n",
        "decision_index = np.argmax(expected_utilities)\n",
        "decision = actions[decision_index]\n",
        "\n",
        "# Output results\n",
        "print(\"Expected Utilities:\", expected_utilities)\n",
        "print(\"Decision:\", decision)"
      ],
      "metadata": {
        "id": "ccklu4m4ZAd8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}